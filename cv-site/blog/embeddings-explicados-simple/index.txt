3:I[39275,[],""]
5:I[61343,[],""]
6:I[84080,["223","static/chunks/223-af2330a6fb348319.js","185","static/chunks/app/layout-d6f26fcc1963b964.js"],""]
7:I[45064,["223","static/chunks/223-af2330a6fb348319.js","185","static/chunks/app/layout-d6f26fcc1963b964.js"],"ThemeProvider"]
8:I[78155,["223","static/chunks/223-af2330a6fb348319.js","185","static/chunks/app/layout-d6f26fcc1963b964.js"],"LanguageProvider"]
9:I[74541,["223","static/chunks/223-af2330a6fb348319.js","185","static/chunks/app/layout-d6f26fcc1963b964.js"],"default"]
4:["slug","embeddings-explicados-simple","d"]
0:["0H7BAbDK5nDkGqgyYYHWC",[[["",{"children":["blog",{"children":[["slug","embeddings-explicados-simple","d"],{"children":["__PAGE__?{\"slug\":\"embeddings-explicados-simple\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["blog",{"children":[["slug","embeddings-explicados-simple","d"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"es","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"preconnect","href":"https://fonts.googleapis.com"}],["$","link",null,{"rel":"preconnect","href":"https://fonts.gstatic.com","crossOrigin":"anonymous"}],["$","link",null,{"href":"https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=JetBrains+Mono:wght@400;500&display=swap","rel":"stylesheet"}],["$","meta",null,{"name":"theme-color","content":"#111827"}],["$","meta",null,{"name":"geo.region","content":"AR-C"}],["$","meta",null,{"name":"geo.placename","content":"Buenos Aires"}]]}],["$","body",null,{"children":[["$","link",null,{"rel":"stylesheet","href":"https://assets.calendly.com/assets/external/widget.css"}],["$","$L6",null,{"src":"https://assets.calendly.com/assets/external/widget.js","strategy":"lazyOnload"}],["$","$L6",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-DG0SLT5RY3","strategy":"afterInteractive"}],["$","$L6",null,{"id":"ga4-init","strategy":"afterInteractive","children":"\n            window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'G-DG0SLT5RY3', {\n              page_path: window.location.pathname,\n              site_section: 'cv',\n              client_name: 'Mariano Gobea Alcoba',\n            });\n          "}],["$","$L7",null,{"children":["$","$L8",null,{"children":["$","$L9",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"styles":null}]}]}]}]]}]]}],null],null],[[["$","link","0",{"rel":"stylesheet","href":"/cv-site/_next/static/css/f8ac64fc2ba06626.css","precedence":"next","crossOrigin":"$undefined"}]],[null,"$La"]]]]]
b:I[53655,["790","static/chunks/790-730599d05686c8ce.js","177","static/chunks/177-ef7799c216a40fc6.js","308","static/chunks/app/blog/%5Bslug%5D/page-76c1c5a125c25d8f.js"],"default"]
c:I[61592,["790","static/chunks/790-730599d05686c8ce.js","177","static/chunks/177-ef7799c216a40fc6.js","308","static/chunks/app/blog/%5Bslug%5D/page-76c1c5a125c25d8f.js"],"default"]
d:I[231,["790","static/chunks/790-730599d05686c8ce.js","177","static/chunks/177-ef7799c216a40fc6.js","308","static/chunks/app/blog/%5Bslug%5D/page-76c1c5a125c25d8f.js"],""]
f:I[65190,["790","static/chunks/790-730599d05686c8ce.js","177","static/chunks/177-ef7799c216a40fc6.js","308","static/chunks/app/blog/%5Bslug%5D/page-76c1c5a125c25d8f.js"],"default"]
e:T1a06,<h2>¿Qué Son Embeddings?</h2>
<p><strong>Vectores numéricos que representan el significado de un texto.</strong></p>
<p>Ejemplo:</p>
<ul>
<li>"perro" → [0.2, -0.5, 0.8, ..., 0.1] (1536 números)</li>
<li>"gato" → [0.18, -0.48, 0.75, ..., 0.09]</li>
</ul>
<p>Palabras similares tienen vectores similares.</p>
<h2>Por Qué Importan</h2>
<p>Computadoras no entienden texto. <strong>Entienden números.</strong></p>
<p>Embeddings convierten texto → números de forma que <strong>preserve el significado</strong>.</p>
<h2>Código: Tu Primer Embedding</h2>
<pre><code class="language-python">import openai

text = "Python es un lenguaje de programación"

response = openai.Embedding.create(
    input=text,
    model="text-embedding-3-small"
)

embedding = response['data'][0]['embedding']

print(f"Dimensiones: {len(embedding)}")  # 1536
print(f"Primeros 5 valores: {embedding[:5]}")
# [0.023, -0.154, 0.892, 0.421, -0.667]
</code></pre>
<h2>Similitud Semántica</h2>
<p>Dos textos similares = embeddings cercanos.</p>
<pre><code class="language-python">import numpy as np

def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

# Embeddings
emb_python = get_embedding("Python es un lenguaje")
emb_java = get_embedding("Java es un lenguaje")
emb_pizza = get_embedding("Me gusta la pizza")

# Similitudes
print(cosine_similarity(emb_python, emb_java))   # 0.87 (muy similar)
print(cosine_similarity(emb_python, emb_pizza))  # 0.12 (no relacionados)
</code></pre>
<h2>Aplicación #1: Search Semántico</h2>
<pre><code class="language-python"># Indexar documentos
docs = [
    "Cómo optimizar queries en BigQuery",
    "Python para automatizar tareas",
    "Guía de Docker para data engineers"
]

embeddings_docs = [get_embedding(doc) for doc in docs]

# Query del usuario
query = "mejorar performance de SQL"
emb_query = get_embedding(query)

# Encontrar doc más similar
similarities = [cosine_similarity(emb_query, emb_doc) for emb_doc in embeddings_docs]
best_match_idx = np.argmax(similarities)

print(f"Mejor match: {docs[best_match_idx]}")
# Output: "Cómo optimizar queries en BigQuery"
</code></pre>
<p><strong>Magia:</strong> Encontró "BigQuery" aunque busqué "SQL".</p>
<h2>Aplicación #2: RAG (Retrieval Augmented Generation)</h2>
<pre><code class="language-python">def ask_with_context(question, knowledge_base):
    # 1. Buscar docs relevantes
    emb_q = get_embedding(question)
    similarities = [cosine_similarity(emb_q, emb_doc) 
                    for emb_doc in knowledge_base_embeddings]
    
    # Top 3 docs
    top_3_idx = np.argsort(similarities)[-3:]
    context = "\n".join([knowledge_base[i] for i in top_3_idx])
    
    # 2. LLM con contexto
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": f"Contexto:\n{context}"},
            {"role": "user", "content": question}
        ]
    )
    
    return response.choices[0].message.content

# Usar
answer = ask_with_context(
    "¿Cómo optimizar BigQuery?",
    knowledge_base=mis_documentos
)
</code></pre>
<h2>Aplicación #3: Clasificación de Texto</h2>
<pre><code class="language-python"># Embeddings de categorías
categorias = {
    "soporte": get_embedding("problema técnico, error, no funciona, ayuda"),
    "ventas": get_embedding("comprar, precio, producto, cotización"),
    "rrhh": get_embedding("empleo, trabajo, vacante, sueldo")
}

# Clasificar email
email_text = "Hola, no puedo iniciar sesión en el sistema"
emb_email = get_embedding(email_text)

# Encontrar categoría más similar
scores = {cat: cosine_similarity(emb_email, emb_cat) 
          for cat, emb_cat in categorias.items()}

categoria_final = max(scores, key=scores.get)
print(f"Categoría: {categoria_final}")  # "soporte"
</code></pre>
<h2>Modelos de Embeddings en 2026</h2>
<p>| Modelo | Dimensiones | Costo/1M tokens | Use Case |
|--------|-------------|-----------------|----------|
| <strong>text-embedding-3-small</strong> | 1536 | $0.02 | General purpose |
| <strong>text-embedding-3-large</strong> | 3072 | $0.13 | Max quality |
| <strong>ada-002</strong> (old) | 1536 | $0.10 | Legacy |</p>
<p><strong>Recomendación:</strong> text-embedding-3-small (mejor calidad/precio).</p>
<h2>Embeddings Multilingües</h2>
<pre><code class="language-python"># Mismo embedding model funciona en múltiples idiomas
emb_es = get_embedding("Hola, cómo estás")
emb_en = get_embedding("Hello, how are you")

similarity = cosine_similarity(emb_es, emb_en)
print(similarity)  # ~0.85 (alta similitud cross-language!)
</code></pre>
<h2>Limitaciones</h2>
<h3>1. Contexto Limitado</h3>
<p>Embeddings capturan significado de <strong>~8000 tokens máximo</strong>.</p>
<p>Para documentos largos: <strong>chunking</strong>.</p>
<h3>2. No Es Perfecto</h3>
<pre><code class="language-python"># Estas dos tienen embeddings similares:
"Amo Python"
"Odio Python"

# Embeddings capturan TÓPICO, no necesariamente sentimiento
</code></pre>
<h3>3. Costo</h3>
<p>1M tokens de embeddings = $0.02</p>
<p>Para millones de docs, el costo escala.</p>
<h2>Tips de Producción</h2>
<h3>1. Cache Embeddings</h3>
<pre><code class="language-python">import hashlib
import pickle

def get_embedding_cached(text, cache_dir='embeddings_cache'):
    # Hash del texto
    text_hash = hashlib.md5(text.encode()).hexdigest()
    cache_path = f"{cache_dir}/{text_hash}.pkl"
    
    # Check cache
    if os.path.exists(cache_path):
        with open(cache_path, 'rb') as f:
            return pickle.load(f)
    
    # Generate embedding
    embedding = get_embedding(text)
    
    # Save to cache
    os.makedirs(cache_dir, exist_ok=True)
    with open(cache_path, 'wb') as f:
        pickle.dump(embedding, f)
    
    return embedding
</code></pre>
<h3>2. Batch Processing</h3>
<pre><code class="language-python"># ✅ Procesar en batch (más eficiente)
texts = ["texto1", "texto2", ..., "texto100"]

response = openai.Embedding.create(
    input=texts,  # Lista de textos
    model="text-embedding-3-small"
)

embeddings = [item['embedding'] for item in response['data']]
</code></pre>
<h2>Conclusión</h2>
<p>Embeddings son la <strong>tecnología fundamental</strong> detrás de GenAI moderno.</p>
<p>Entenderlos te permite construir:</p>
<ul>
<li>Search semántico</li>
<li>RAG systems</li>
<li>Clasificadores de texto</li>
<li>Recomendadores</li>
</ul>
<p>No es magia: <strong>es matemática bien aplicada</strong>.</p>
<hr>
<p><em>¿Implementando search semántico? <a href="https://mgobeaalcoba.github.io/blog-post.html?slug=vector-databases-guia-practica">Vector DB post</a></em></p>
2:["$","main",null,{"className":"min-h-screen","children":[["$","$Lb",null,{}],["$","$Lc",null,{}],["$","article",null,{"className":"max-w-3xl mx-auto px-4 sm:px-6 lg:px-8 pt-28 pb-20","children":[["$","$Ld",null,{"href":"/blog/","className":"inline-flex items-center gap-2 text-sm text-gray-400 hover:text-sky-400 transition-colors mb-8","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":14,"height":14,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-arrow-left","aria-hidden":"true","children":[["$","path","1l729n",{"d":"m12 19-7-7 7-7"}],["$","path","x3x0zl",{"d":"M19 12H5"}],"$undefined"]}],"Volver al blog"]}],["$","header",null,{"className":"mb-10","children":[["$","div",null,{"className":"flex flex-wrap items-center gap-3 mb-4","children":[["$","span",null,{"className":"text-xs px-2 py-0.5 bg-sky-500/20 text-sky-300 border border-sky-500/30 rounded-full font-medium","children":"automation"}],["$","span",null,{"className":"text-xs px-2 py-0.5 bg-yellow-500/20 text-yellow-300 border border-yellow-500/30 rounded-full","children":"⭐ Featured"}]]}],["$","h1",null,{"className":"text-3xl sm:text-4xl font-black text-gray-100 leading-tight mb-4","children":"Embeddings explicados simple: La magia detrás de GenAI"}],["$","p",null,{"className":"text-gray-400 text-lg leading-relaxed mb-6","children":"Qué son embeddings, cómo funcionan y por qué son la tecnología clave detrás de GenAI."}],["$","div",null,{"className":"flex flex-wrap items-center gap-4 text-sm text-gray-500 pb-6 border-b border-white/10","children":[["$","span",null,{"className":"flex items-center gap-1.5","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":14,"height":14,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-calendar","aria-hidden":"true","children":[["$","path","1cmpym",{"d":"M8 2v4"}],["$","path","4m81vk",{"d":"M16 2v4"}],["$","rect","1hopcy",{"width":"18","height":"18","x":"3","y":"4","rx":"2"}],["$","path","8toen8",{"d":"M3 10h18"}],"$undefined"]}],"20 de junio de 2026"]}],["$","span",null,{"className":"flex items-center gap-1.5","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":14,"height":14,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-clock","aria-hidden":"true","children":[["$","circle","1mglay",{"cx":"12","cy":"12","r":"10"}],["$","path","mmk7yg",{"d":"M12 6v6l4 2"}],"$undefined"]}],"10 min"]}],["$","div",null,{"className":"flex gap-2 flex-wrap","children":[["$","span","embeddings",{"className":"flex items-center gap-1 text-xs text-gray-400","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":10,"height":10,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-tag","aria-hidden":"true","children":[["$","path","vktsd0",{"d":"M12.586 2.586A2 2 0 0 0 11.172 2H4a2 2 0 0 0-2 2v7.172a2 2 0 0 0 .586 1.414l8.704 8.704a2.426 2.426 0 0 0 3.42 0l6.58-6.58a2.426 2.426 0 0 0 0-3.42z"}],["$","circle","kqv944",{"cx":"7.5","cy":"7.5","r":".5","fill":"currentColor"}],"$undefined"]}],"embeddings"]}],["$","span","genai",{"className":"flex items-center gap-1 text-xs text-gray-400","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":10,"height":10,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-tag","aria-hidden":"true","children":[["$","path","vktsd0",{"d":"M12.586 2.586A2 2 0 0 0 11.172 2H4a2 2 0 0 0-2 2v7.172a2 2 0 0 0 .586 1.414l8.704 8.704a2.426 2.426 0 0 0 3.42 0l6.58-6.58a2.426 2.426 0 0 0 0-3.42z"}],["$","circle","kqv944",{"cx":"7.5","cy":"7.5","r":".5","fill":"currentColor"}],"$undefined"]}],"genai"]}],["$","span","ml",{"className":"flex items-center gap-1 text-xs text-gray-400","children":[["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":10,"height":10,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-tag","aria-hidden":"true","children":[["$","path","vktsd0",{"d":"M12.586 2.586A2 2 0 0 0 11.172 2H4a2 2 0 0 0-2 2v7.172a2 2 0 0 0 .586 1.414l8.704 8.704a2.426 2.426 0 0 0 3.42 0l6.58-6.58a2.426 2.426 0 0 0 0-3.42z"}],["$","circle","kqv944",{"cx":"7.5","cy":"7.5","r":".5","fill":"currentColor"}],"$undefined"]}],"ml"]}]]}]]}]]}],["$","div",null,{"className":"prose prose-invert prose-sky max-w-none prose-headings:text-gray-100 prose-headings:font-bold prose-p:text-gray-300 prose-p:leading-relaxed prose-a:text-sky-400 prose-a:no-underline hover:prose-a:underline prose-code:text-sky-300 prose-code:bg-sky-500/10 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-pre:bg-gray-900 prose-pre:border prose-pre:border-white/10 prose-blockquote:border-l-sky-500 prose-blockquote:text-gray-400 prose-strong:text-gray-100 prose-li:text-gray-300 prose-hr:border-white/10","dangerouslySetInnerHTML":{"__html":"$e"}}],["$","div",null,{"className":"mt-12 pt-8 border-t border-white/10","children":[["$","p",null,{"className":"text-gray-400 text-sm mb-4","children":["Escrito por ",["$","span",null,{"className":"text-sky-400 font-medium","children":"Mariano Gobea Alcoba"}]]}],["$","div",null,{"className":"flex gap-4","children":[["$","$Ld",null,{"href":"/blog/","className":"text-sm text-sky-400 hover:text-sky-300 transition-colors","children":"← Otros artículos"}],["$","$Ld",null,{"href":"/consulting/","className":"text-sm text-sky-400 hover:text-sky-300 transition-colors","children":"Consultoría →"}]]}]]}]]}],["$","$Lf",null,{}]]}]
a:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Embeddings explicados simple: La magia detrás de GenAI | Mariano Gobea Alcoba"}],["$","meta","3",{"name":"description","content":"Qué son embeddings, cómo funcionan y por qué son la tecnología clave detrás de GenAI."}],["$","link","4",{"rel":"author","href":"https://mgobeaalcoba.github.io"}],["$","meta","5",{"name":"author","content":"Mariano Gobea Alcoba"}],["$","meta","6",{"name":"keywords","content":"data analytics,technical leader,mercadolibre,mariano gobea alcoba,consultoría tecnológica,automatización,business intelligence,python,machine learning,data engineer,buenos aires"}],["$","meta","7",{"name":"creator","content":"Mariano Gobea Alcoba"}],["$","meta","8",{"name":"robots","content":"index, follow"}],["$","meta","9",{"name":"googlebot","content":"index, follow"}],["$","meta","10",{"property":"og:title","content":"Embeddings explicados simple: La magia detrás de GenAI"}],["$","meta","11",{"property":"og:description","content":"Qué son embeddings, cómo funcionan y por qué son la tecnología clave detrás de GenAI."}],["$","meta","12",{"property":"og:type","content":"article"}],["$","meta","13",{"property":"article:published_time","content":"2026-06-21"}],["$","meta","14",{"property":"article:author","content":"Mariano Gobea Alcoba"}],["$","meta","15",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","16",{"name":"twitter:site","content":"@MGobeaAlcoba"}],["$","meta","17",{"name":"twitter:creator","content":"@MGobeaAlcoba"}],["$","meta","18",{"name":"twitter:title","content":"Embeddings explicados simple: La magia detrás de GenAI"}],["$","meta","19",{"name":"twitter:description","content":"Qué son embeddings, cómo funcionan y por qué son la tecnología clave detrás de GenAI."}],["$","link","20",{"rel":"icon","href":"/cv-site/favicon.ico","type":"image/x-icon","sizes":"16x16"}]]
1:null
